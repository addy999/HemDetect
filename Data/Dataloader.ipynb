{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import _pickle as pickle \n",
    "import torch\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, path_to_pickle_folders, replace_classes = {}, maximum_per_folder = None):\n",
    "        \n",
    "        if type(path_to_pickle_folders) != list:\n",
    "            path_to_pickle_folders = [path_to_pickle_folders]\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        for folder in path_to_pickle_folders:\n",
    "            print(\"Unpacking\", os.path.basename(folder))\n",
    "            working_label = os.path.basename(folder)\n",
    "            \n",
    "            if os.path.basename(folder) in replace_classes:\n",
    "                working_label = replace_classes[os.path.basename(folder)]\n",
    "            \n",
    "            files_to_unpickle = [os.path.join(folder, img) for img in os.listdir(folder)]\n",
    "            files_to_unpickle = files_to_unpickle[:maximum_per_folder]\n",
    "            \n",
    "#             Single threaded\n",
    "#             for img in files_to_unpickle:\n",
    "#                 self.addPickle(img)\n",
    "            \n",
    "#             Multithreaded\n",
    "            p = Pool()\n",
    "            results = p.map(self.parsePickle, files_to_unpickle)\n",
    "            p.close()\n",
    "            p.join()      \n",
    "            \n",
    "            # add to data\n",
    "            for file in results:\n",
    "                try:\n",
    "                    if file:\n",
    "                        pass\n",
    "                except:\n",
    "                    self.data.append({\n",
    "                        working_label : file\n",
    "                    })\n",
    "\n",
    "        self.convetLabels()\n",
    "        self.dataToTensor()\n",
    "    \n",
    "    def dataToTensor(self):\n",
    "        i = 0\n",
    "        for data_dict in self.data:\n",
    "            array = list(data_dict.values())[0]\n",
    "            array = torch.Tensor(array).unsqueeze(0)\n",
    "            self.data[i] = {\n",
    "                list(data_dict.keys())[0] : array\n",
    "            }\n",
    "            i+=1\n",
    "    \n",
    "    def convetLabels(self):\n",
    "        all_labels = np.array([list(data.keys())[0] for data in self.data])\n",
    "        unique_labels = list(np.unique(all_labels))\n",
    "        self.label_dict = {label:unique_labels.index(label) for label in unique_labels}\n",
    "    \n",
    "    def parsePickle(self, path_to_pickle):\n",
    "        try:\n",
    "            f=open(path_to_pickle,'rb')\n",
    "            img=pickle.load(f)\n",
    "            f.close()\n",
    "            return img\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Return img, label'''\n",
    "        data = self.data[idx]\n",
    "        img = list(data.values())[0]\n",
    "        word_label = list(data.keys())[0]\n",
    "        label = self.label_dict[word_label]\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: Load hem/noHem classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking epidural\n",
      "Unpacking intraparenchymal\n",
      "Unpacking subarachnoid\n",
      "Unpacking intraventricular\n",
      "Unpacking subdural\n",
      "Unpacking nohem\n",
      "CPU times: user 2.43 s, sys: 3.44 s, total: 5.88 s\n",
      "Wall time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_folders = [\n",
    "    \"Processed/train/epidural\",\n",
    "    \"Processed/train/intraparenchymal\",\n",
    "    \"Processed/train/subarachnoid\",\n",
    "    \"Processed/train/intraventricular\",\n",
    "    \"Processed/train/subdural\",\n",
    "    \"Processed/train/nohem\",\n",
    "]\n",
    "\n",
    "train_data = Data(training_folders, replace_classes = {\n",
    "    \"epidural\":\"any\", \n",
    "    \"intraparenchymal\":\"any\", \n",
    "    \"subarachnoid\":\"any\", \n",
    "    \"intraventricular\":\"any\", \n",
    "    \"subdural\":\"any\", \n",
    "}, maximum_per_folder=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[-1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: Load 5 sub classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folders = [\n",
    "    \"Processed/train/epidural\",\n",
    "    \"Processed/train/intraparenchymal\",\n",
    "    \"Processed/train/subarachnoid\",\n",
    "    \"Processed/train/intraventricular\",\n",
    "    \"Processed/train/subdural\",\n",
    "]\n",
    "\n",
    "train_data = Data(training_folders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
