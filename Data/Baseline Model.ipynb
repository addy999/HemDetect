{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f438992f590>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 512 * 512\n",
    "dim_h = 32\n",
    "dim_out = 6\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim_x, dim_h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(dim_h, dim_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, val_data_loader=None, learning_rate=0.001, num_epoch=1, plot=False):\n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.7)\n",
    "\n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    epoch = 0 \n",
    "    iteration = 0\n",
    "    while (epoch < num_epoch):\n",
    "        for batch_id, (data, target) in enumerate(train_data_loader):\n",
    "            #############################################\n",
    "            #To Enable GPU Usage\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            #############################################\n",
    "            out = model(data)             # forward pass\n",
    "            loss = criterion(out, target) # compute the total loss\n",
    "            loss.backward()               # backward pass (compute parameter updates)\n",
    "            optimizer.step()              # make the updates for each parameter\n",
    "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
    "            # Save the current model (checkpoint) to a file\n",
    "#             model_path = get_model_name(model.name, learning_rate, epoch)\n",
    "#             torch.save(model.state_dict(), model_path)\n",
    "        losses.append(float(loss))\n",
    "        iters.append(iteration)\n",
    "        if epoch%5==0:\n",
    "            print(\"Epoch: \" + str(epoch) + \" Loss: \" + str(losses[-1]))\n",
    "        if plot:\n",
    "            # save the current training information\n",
    "            train_acc.append(get_accuracy(train_data_loader, model=model))\n",
    "            if val_data_loader is not None:\n",
    "                val_acc.append(get_accuracy(val_data_loader, model=model))\n",
    "        iteration += 1\n",
    "        epoch += 1\n",
    "#         print(epoch) \n",
    "    \n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    if plot:\n",
    "        plt.title(\"Training Curve\")\n",
    "        plt.plot(iters, train_acc, label=\"Train\")\n",
    "        if val_data_loader is not None:\n",
    "            plt.plot(iters, val_acc, label=\"Validation\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Training Accuracy\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data_loader, model=None, out=None):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for imgs, labels in data_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            imgs = imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        if out is not None:\n",
    "            output = out\n",
    "        else:\n",
    "            output = model(imgs)\n",
    "        #select index with maximum prediction score\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import _pickle as pickle \n",
    "import torch\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, path_to_pickle_folders, replace_classes = {}, maximum_per_folder = None):\n",
    "        \n",
    "        if type(path_to_pickle_folders) != list:\n",
    "            path_to_pickle_folders = [path_to_pickle_folders]\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        for folder in path_to_pickle_folders:\n",
    "            print(\"Unpacking\", os.path.basename(folder))\n",
    "            working_label = os.path.basename(folder)\n",
    "            \n",
    "            if os.path.basename(folder) in replace_classes:\n",
    "                working_label = replace_classes[os.path.basename(folder)]\n",
    "            \n",
    "            files_to_unpickle = [os.path.join(folder, img) for img in os.listdir(folder)]\n",
    "            files_to_unpickle = files_to_unpickle[:maximum_per_folder]\n",
    "            \n",
    "#             Single threaded\n",
    "#             for img in files_to_unpickle:\n",
    "#                 self.addPickle(img)\n",
    "            \n",
    "#             Multithreaded\n",
    "            p = Pool()\n",
    "            results = p.map(self.parsePickle, files_to_unpickle)\n",
    "            p.close()\n",
    "            p.join()      \n",
    "            \n",
    "            # add to data\n",
    "            for file in results:\n",
    "                self.data.append({\n",
    "                    working_label : file\n",
    "                })\n",
    "\n",
    "        self.convetLabels()\n",
    "        self.dataToTensor()\n",
    "    \n",
    "    def dataToTensor(self):\n",
    "        i = 0\n",
    "        for data_dict in self.data:\n",
    "            array = list(data_dict.values())[0]\n",
    "            array = torch.Tensor(array).unsqueeze(0)\n",
    "            self.data[i] = {\n",
    "                list(data_dict.keys())[0] : array\n",
    "            }\n",
    "            i+=1\n",
    "    \n",
    "    def convetLabels(self):\n",
    "        all_labels = np.array([list(data.keys())[0] for data in self.data])\n",
    "        unique_labels = list(np.unique(all_labels))\n",
    "        self.label_dict = {label:unique_labels.index(label) for label in unique_labels}\n",
    "    \n",
    "    def parsePickle(self, path_to_pickle):\n",
    "        try:\n",
    "            f=open(path_to_pickle,'rb')\n",
    "            img=pickle.load(f)\n",
    "            f.close()\n",
    "            return img\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Return img, label'''\n",
    "        data = self.data[idx]\n",
    "        img = list(data.values())[0]\n",
    "        word_label = list(data.keys())[0]\n",
    "        label = self.label_dict[word_label]\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking epidural\n",
      "Unpacking intraparenchymal\n"
     ]
    }
   ],
   "source": [
    "training_folders = [\n",
    "    \"Processed/train/epidural\",\n",
    "    \"Processed/train/intraparenchymal\",\n",
    "    \"Processed/train/subarachnoid\",\n",
    "    \"Processed/train/intraventricular\",\n",
    "    \"Processed/train/subdural\",\n",
    "    \"Processed/train/nohem\",\n",
    "]\n",
    "\n",
    "train_data = Data(training_folders, replace_classes = {\n",
    "    \"epidural\":\"any\", \n",
    "    \"intraparenchymal\":\"any\", \n",
    "    \"subarachnoid\":\"any\", \n",
    "    \"intraventricular\":\"any\", \n",
    "    \"subdural\":\"any\", \n",
    "}, maximum_per_folder = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[-1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
