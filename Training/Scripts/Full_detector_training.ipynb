{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={1:1, 10:2, 3:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_a = {i:a[i]for i in sorted(a)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 3: 3, 10: 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import _pickle as pickle \n",
    "import torch\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, path_to_pickle_folders, replace_classes = {}, maximum_per_folder = None):\n",
    "        \n",
    "        if type(path_to_pickle_folders) != list:\n",
    "            path_to_pickle_folders = [path_to_pickle_folders]\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        for folder in path_to_pickle_folders:\n",
    "            print(\"Unpacking\", os.path.basename(folder))\n",
    "            working_label = os.path.basename(folder)\n",
    "            \n",
    "            if os.path.basename(folder) in replace_classes:\n",
    "                working_label = replace_classes[os.path.basename(folder)]\n",
    "            \n",
    "            files_to_unpickle = [os.path.join(folder, img) for img in os.listdir(folder)]\n",
    "            files_to_unpickle = files_to_unpickle[:maximum_per_folder]\n",
    "            \n",
    "#             gc.disable()\n",
    "            p = Pool()\n",
    "            results = p.map(self.parsePickle, files_to_unpickle)\n",
    "            p.close()\n",
    "            p.join()      \n",
    "#             gc.enable()\n",
    "            \n",
    "            # add to data\n",
    "            for file in results:\n",
    "                try:\n",
    "                    if file:\n",
    "                        pass\n",
    "                except:\n",
    "                    self.data.append({\n",
    "                        working_label : file\n",
    "                    })\n",
    "\n",
    "        self.convetLabels()\n",
    "        self.dataToTensor()\n",
    "    \n",
    "    def dataToTensor(self):\n",
    "        i = 0\n",
    "        for data_dict in self.data:\n",
    "            array = list(data_dict.values())[0]\n",
    "            array = torch.Tensor(array).unsqueeze(0)\n",
    "            self.data[i] = {\n",
    "                list(data_dict.keys())[0] : array\n",
    "            }\n",
    "            i+=1\n",
    "    \n",
    "    def convetLabels(self):\n",
    "        all_labels = np.array([list(data.keys())[0] for data in self.data])\n",
    "        unique_labels = list(np.unique(all_labels))\n",
    "        self.label_dict = {label:unique_labels.index(label) for label in unique_labels}\n",
    "    \n",
    "    def parsePickle(self, path_to_pickle):\n",
    "        try:\n",
    "            f=open(path_to_pickle,'rb')\n",
    "            \n",
    "            gc.disable()\n",
    "            img=pickle.load(f)\n",
    "            gc.enable()\n",
    "            \n",
    "            f.close()\n",
    "            return img\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Return img, label'''\n",
    "        data = self.data[idx]\n",
    "        img = list(data.values())[0]\n",
    "        word_label = list(data.keys())[0]\n",
    "        label = self.label_dict[word_label]\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUcLcj3Y-rpo"
   },
   "source": [
    "## **Hemorrhage Classifier:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ay1o4mzuYrV"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "\n",
    "alexnet_model = torchvision.models.alexnet(pretrained=True)\n",
    "alexnet_model.features[0] = nn.Conv2d(1, 64, kernel_size= 7, stride= 2, padding= 3)\n",
    "\n",
    "class HemorrhageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HemorrhageClassifier, self).__init__()\n",
    "        self.name = \"Classifier\"\n",
    "\n",
    "        for param in alexnet_model.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 512 * 1, 100)\n",
    "        self.fc2 = nn.Linear(100, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = alexnet_model.features(x)\n",
    "        x = x.view(-1, 512 * 512 * 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAMPFBo6mn12"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "\n",
    "resnet152 = torchvision.models.resnet152(pretrained=True)\n",
    "modules = list(resnet152.children())\n",
    "modules[0].in_channels = 1\n",
    "\n",
    "class HemorrhageClassifier2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HemorrhageClassifier2, self).__init__()\n",
    "        self.name = \"Classifier 2\"\n",
    "\n",
    "        for param in resnet152.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 512 * 1, 100)\n",
    "        self.fc2 = nn.Linear(100, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = resnet152(x)\n",
    "        x = x.view(-1, 512 * 512 * 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HX23rHLw-zvs"
   },
   "source": [
    "# **Hemorrhage Detector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wYexXEgE6P3"
   },
   "source": [
    "**Two convulational layers:**\n",
    "  - each have a 5x5 receptive field, 2x2 stride, and 2x2 padding\n",
    "\n",
    "**The number of feature maps:**\n",
    "  - for the 1st convulational layer is 10\n",
    "  - for the 2nd convolutional layer is 20\n",
    "\n",
    "**Two Pooling Layers:**\n",
    "  - Receptive field: 2x2, stride: 2x2\n",
    "\n",
    "**Fully Connected layer:**\n",
    "  - 100 nodes\n",
    "  - 2 output nodes (binary classification)\n",
    "\n",
    "**Training Parameters:**\n",
    "\n",
    "lr = 0.02\n",
    "momentum =0.01\n",
    "4000 iterations (60 epochs)\n",
    "Batch size of 20\n",
    "Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "GAEyk5RdXvo-",
    "outputId": "31bc541f-7d92-4d06-9581-d4099b1c9a6b"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "\n",
    "alexnet_model = torchvision.models.alexnet(pretrained=True)\n",
    "alexnet_model.features[0] = nn.Conv2d(1, 64, kernel_size= 7, stride= 2, padding= 3)\n",
    "\n",
    "class HemorrhageDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HemorrhageDetector, self).__init__()\n",
    "        self.name = \"Detector\"\n",
    "\n",
    "        for param in alexnet_model.parameters():\n",
    "              param.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(256*31*31, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = alexnet_model.features(x)\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 256*31*31)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBkK6sUWiDj4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "\n",
    "resnet152 = torchvision.models.resnet152(pretrained=True)\n",
    "modules = list(resnet152.children())\n",
    "modules[0].in_channels = 1\n",
    "\n",
    "class HemorrhageDetector2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HemorrhageDetector2, self).__init__()\n",
    "        self.name = \"Detector 2\"\n",
    "\n",
    "        for param in resnet152.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 512 * 1, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = resnet152(x)\n",
    "        x = x.view(-1, 512 * 512 * 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "163ShF1WqWEw"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjV2sU8qqVd0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_accuracy(model, data_loader, use_cuda):\n",
    "\n",
    "    cor = 0\n",
    "    total = 0\n",
    "    n = 0\n",
    "    for imgs, labels in data_loader:\n",
    "#         imgs = features = torch.load(f'{name}_features_batchcount{n}.tensor')\n",
    "        imgs = torch.from_numpy(imgs.detach().numpy())\n",
    "        #To Enable GPU Usage\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            imgs = imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        #############################################\n",
    "        output = model(imgs)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        cor = cor + pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total = total + imgs.shape[0]\n",
    "        n = n+1\n",
    "    return cor / total\n",
    "\n",
    "def train(model, train_dataset, val_dataset, batch_size = 64, learning_rate=0.01, num_epochs=30, use_cuda = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    \n",
    "    training_loader = torch.utils.data.DataLoader(train_dataset, batch_size= batch_size)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size= 32)\n",
    "\n",
    "    n = 0 \n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        for imgs, labels in iter(training_loader):\n",
    "#             imgs = torch.load(f'training_features_batchcount{count}.tensor')\n",
    "            imgs = torch.from_numpy(imgs.detach().numpy())\n",
    "\n",
    "            #To Enable GPU Usage\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                imgs = imgs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            #############################################\n",
    "            \n",
    "            outputs = model(imgs)             \n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward()               \n",
    "            optimizer.step()              \n",
    "            optimizer.zero_grad()         \n",
    "\n",
    "            # save the current training information\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss)/batch_size)             # finding the average loss\n",
    "#             train_acc.append(get_accuracy(model, training_loader, use_cuda)) # finding the training accuracy \n",
    "#             val_acc.append(get_accuracy(model, val_loader, use_cuda))  # finding the validation accuracy\n",
    "            n = n+1\n",
    "            count = count+1\n",
    "        \n",
    "        print(\"Epoch\", epoch, \"Loss\", loss)\n",
    "        \n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = \"Models/model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "          \n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), losses)\n",
    "\n",
    "#     plt.title(\"Training Loss Curve\")\n",
    "#     plt.plot(iters, losses, label=\"Train\")\n",
    "#     plt.xlabel(\"Iterations\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.title(\"Training vs. Validation Accuracy Curves\")\n",
    "#     plt.plot(iters, train_acc, label=\"Train\")\n",
    "#     plt.plot(iters, val_acc, label=\"Validation\")\n",
    "#     plt.xlabel(\"Iterations\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Unpacking epidural\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 363, in put\n",
      "    with self._wlock:\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 127, in worker\n",
      "    put((job, i, result))\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 364, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e8f2a5c2231a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"subarachnoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"any\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"intraventricular\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"any\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;34m\"subdural\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"any\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m })\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-248d27631818>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_pickle_folders, replace_classes, maximum_per_folder)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#             gc.disable()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsePickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_to_unpickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Train...\")\n",
    "\n",
    "training_folders = [\n",
    "    \"../Data/Processed/train/epidural\",\n",
    "    \"../Data/Processed/train/intraparenchymal\",\n",
    "    \"../Data/Processed/train/subarachnoid\",\n",
    "    \"../Data/Processed/train/intraventricular\",\n",
    "    \"../Data/Processed/train/subdural\",\n",
    "    \"../Data/Processed/train/nohem\",\n",
    "]\n",
    "\n",
    "train_data = Data(training_folders, replace_classes = {\n",
    "    \"epidural\":\"any\", \n",
    "    \"intraparenchymal\":\"any\", \n",
    "    \"subarachnoid\":\"any\", \n",
    "    \"intraventricular\":\"any\", \n",
    "    \"subdural\":\"any\", \n",
    "})\n",
    "\n",
    "print(\"Val....\")\n",
    "\n",
    "val_folders = [\n",
    "    \"../Data/Processed/val/epidural\",\n",
    "    \"../Data/Processed/val/intraparenchymal\",\n",
    "    \"../Data/Processed/val/subarachnoid\",\n",
    "    \"../Data/Processed/val/intraventricular\",\n",
    "    \"../Data/Processed/val/subdural\",\n",
    "    \"../Data/Processed/val/nohem\",\n",
    "]\n",
    "\n",
    "val_data = Data(val_folders, replace_classes = {\n",
    "    \"epidural\":\"any\", \n",
    "    \"intraparenchymal\":\"any\", \n",
    "    \"subarachnoid\":\"any\", \n",
    "    \"intraventricular\":\"any\", \n",
    "    \"subdural\":\"any\", \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amound of train+val data being used:\", len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HemorrhageDetector()\n",
    "train(model, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classfier run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_folders = [\n",
    "#     \"Processed/train/epidural\",\n",
    "#     \"Processed/train/intraparenchymal\",\n",
    "#     \"Processed/train/subarachnoid\",\n",
    "#     \"Processed/train/intraventricular\",\n",
    "#     \"Processed/train/subdural\",\n",
    "# ]\n",
    "\n",
    "# train_data = Data(training_folders)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN Architecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}